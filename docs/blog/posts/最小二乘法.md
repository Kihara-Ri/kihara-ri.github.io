---
title: 最小二乘法
date: 2024-03-03
tags: 
  - 最小二乘法
  - 损失函数
categories: 
  - Math
---

# 最小二乘法

[最小二乘法](https://www.wikiwand.com/en/Least_squares?wprov=srpw1_0)(Ordinary Least Squares, OLS)最早由[勒让德](https://www.wikiwand.com/en/Adrien-Marie_Legendre)(Adrien-Marie Legendre)和数学王子[高斯](https://www.wikiwand.com/en/Carl_Friedrich_Gauss)(Johann Carl Friedrich Gauss)分别发现并使用

对于我们最常用的两个变量之间的回归分析$\hat y = \beta_0+\beta_1 \hat x$有：
$$
\begin{align}
\hat\beta_1 &= \frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2} \\
\hat\beta_0&= \bar y -\hat\beta_1 \bar x
\end{align}
$$

## 二维变量最小二乘法的证明

对于一组数据：
$$
X = \{(x_1,y_1),(x_2,y_2),\cdots , (x_m,y_m)\}
$$
我们认为它现在大致符合一个线性方程：
$$
y = \beta_0 +\beta_1x
$$
也就是我们现在想要用这个线性方程来拟合这组数据，拟合出来的直线要尽大可能地反应出数据的真实分布情况，凭我们朴素的直觉当然是不具有说服力的，因此我们需要找到一种更加客观的方法

我们需要表达出预测值与真实值之间的差距，那么如何表达误差呢？

对于其中任一数据$(x_i,y_i)$，其误差
$$
\Delta = y_i - (\beta_0+\beta_1x_i)
$$
显然，正误差与负误差所表达的意义变得完全不同了，这显然是不行的，考虑增加绝对值？
$$
\Delta =\vert y_i - (\beta_0+\beta_1x_i)\vert
$$
这种做法看似能行，但当我们累计误差时：
$$
J(\beta) = \sum_{i=1}^m\vert y_i - (\beta_0+\beta_1x_i)\vert
$$
我们马上就能看出问题了，这种表达方法是不方便计算的，对于各种正负误差，如果都要进行分类讨论，那也未免太过繁琐了，而且由于存在过多拐点，可导性也很差

不妨取平方！（深度学习中的误差函数也采用了这种方法）

于是
$$
J(\beta) = \sum_{i=1}^m (y_i - \beta_0-\beta_1x_i)^2
$$
现在我们就得到了这样一个量化损失的损失函数，只需要让$\beta_0$和$\beta_1$取得适当的值，让总体的损失函数值最小即可

分别对$\beta_0$和$\beta_1$求偏导：
$$
\frac{\partial J(\beta)}{\partial \beta_0} = \sum_{i=1}^{m}2·(-1)(y_i - \beta_0-\beta_1x_i) = 2\sum_{i=1}^{m}(\beta_1x_i + \beta_0 - y_i) \tag{1}
$$

$$
\frac{\partial J(\beta)}{\partial \beta_1} = \sum_{i=1}^{m}2·(-x_i)(y_i - \beta_0-\beta_1x_i) = 2\sum_{i=1}^{m}(\beta_1x_i^2 + \beta_0x_i - x_iy_i) \tag{2}
$$

我们知道
$$
\left\{
\begin{align}
\bar x = \frac{\sum_{i=1}^{m}x_i}{m}\\
\bar y = \frac{\sum_{i=1}^{m}y_i}{m}
\end{align}
\right.
$$
则
$$
\begin{align}
式(1) &= 2(\beta_1\sum_{i=1}^{m}x_i + \sum_{i=1}^{m}\beta_0 - \sum_{i=1}^{m}y_i)\\
 &= 2(m\beta_1\frac{\sum_{i=1}^{m}x_i}{m} + m\beta_0 - m\frac{\sum_{i=1}^{m}y_i}{m})\\
 &= 2(m\beta_1\bar x + m\beta_0 - m\bar y)\\
 &= 2m(\beta_1\bar x +\beta_0 - \bar y)
\end{align}
$$
令$\frac{\partial J(\beta)}{\partial\beta_0} = 0$，则
$$
\beta_0 = \bar y - \beta_1 \bar x \tag{3}
$$
我们将(3)代入(2)中有：
$$
(2)= 2 \sum_{i=1}^m(\beta_1x_i^2 + (\bar y -\beta_1 \bar x)x_i -x_iy_i) = 0
$$
可以求出
$$
\beta_1 = \frac{\sum_{i=1}^mx_iy_i - \bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx_i^2 - \bar x\sum_{i=1}^mx_i}  \tag{4}
$$
这里我们有求和性质：
$$
\sum_{i=1}^m(x_i - \bar x)(y_i - \bar y) = \sum_{i=1}^mx_iy_i - m\bar x\bar y = \sum_{i=1}^mx_iy_i - \bar y\sum_{i=1}^mx_i
$$
有了这个性质，代入(4)有
$$
\beta_1 = \frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2} \tag{*}
$$


因此，对
$$
\hat y = \beta_0+\beta_1 \hat x 
$$
有：
$$
\left\{
\begin{align}
\hat\beta_1 &= \frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2} \\
\hat\beta_0&= \bar y -\hat\beta_1 \bar x
\end{align}
\right.
$$

## 多元线性回归

对于多维数据，我们有：
$$
X\beta = Y
$$
其中：
$$
X = 
\begin{bmatrix}
{1}&{x_{12}}&{\cdots}&{x_{1n}}\\
{1}&{x_{22}}&{\cdots}&{x_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{1}&{x_{m2}}&{\cdots}&{x_{mn}}\\
\end{bmatrix}
,\quad
\beta = 
\begin{bmatrix}
\beta_0 \\ 
\beta_1 \\
\vdots \\
\beta_n
\end{bmatrix}	
,\quad
y = 
\begin{bmatrix}
y_1 \\ 
y_2 \\
\vdots \\
y_m
\end{bmatrix}	\\
\qquad \qquad \qquad m\times n\qquad\qquad\qquad n\times 1 \qquad\quad m\times 1
$$
目标函数
$$
\begin{align}
J(\beta) &= \sum_{i=1}^m \vert y_i - \sum_{j=1}^nx_{ij}\beta_j\vert^2\\
&= \begin{Vmatrix} Y-X\beta \end{Vmatrix}	^2
\end{align}
$$

$$
\begin{align}
L(D,B) &= \begin{Vmatrix} Y-X\beta \end{Vmatrix}	^2 \\
&= (Y-X\beta)^T(Y-X\beta) \\
&= Y^TY-Y^TX\beta-\beta^TX^TY+\beta^TX^TX\beta
\end{align}
$$

$$
\begin{align}
\frac{\partial L(D,B)}{\partial\beta} &= \frac{\partial(Y^TY-Y^TX\beta-\beta^TX^TY+\beta^TX^TX\beta)}{\partial\beta}\\
&= -2X^TY + 2X^TX\beta = 0
\end{align}
$$

此时可以得到
$$
\hat\beta = (X^TX)^{-1}X^TY
$$

## 一点题外话

我们在分析最小二乘法的误差计算时，考虑的误差差值采取的是直接相减的方法，在简单情况下，我们以二维数据为例，也就是$y$方向的差值，这意味着我们的差值计算是纵向的，这与点到回归曲线的距离计算得出的结果是否相同呢？
